{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.sparse import coo_matrix\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data for IMDB and yelp\n",
    "IT = pd.read_csv('IMDB-train-prep.txt',sep = '\\t',header = None)\n",
    "YT = pd.read_csv('yelp-train-prep.txt',sep = '\\t',header = None)\n",
    "\n",
    "# read validation data\n",
    "IV = pd.read_csv('IMDB-valid-prep.txt',sep = '\\t',header = None)\n",
    "YV = pd.read_csv('yelp-valid-prep.txt',sep = '\\t',header = None)\n",
    "\n",
    "# read test data\n",
    "ITe = pd.read_csv('IMDB-test-prep.txt',sep = '\\t',header = None)\n",
    "YTe = pd.read_csv('yelp-test-prep.txt',sep = '\\t',header = None)\n",
    "\n",
    "# read vocab\n",
    "Iv = pd.read_csv('IMDB-vocab.txt',sep = '\\t',header = None)\n",
    "Yv = pd.read_csv('yelp-vocab.txt',sep = '\\t',header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_duplicates(l):\n",
    "#     newlist = []\n",
    "#     for el in l:\n",
    "#        if el not in newlist:\n",
    "#            newlist.append(el)\n",
    "#     return newlist\n",
    "\n",
    "# # bag of words\n",
    "# def bow(df):\n",
    "# #   extract all non-zero index\n",
    "#     col = []\n",
    "#     row = []\n",
    "#     for i in range(0,len(df[0])):\n",
    "#         try:\n",
    "#             review = remove_duplicates(df[0][i].split(' '))\n",
    "#             col = np.append(col,review)\n",
    "#             wc = np.full(len(review),i)\n",
    "#             row = np.append(row, wc)\n",
    "#         except:\n",
    "#             continue\n",
    "    \n",
    "#     col = list(map(int, col))\n",
    "#     vMatrix = coo_matrix((np.ones(len(row)), (row, col)), shape=(len(df), 10000))\n",
    "#     return vMatrix\n",
    "\n",
    "# # bag of frequency\n",
    "# def bof(df):\n",
    "#     col = []\n",
    "#     row = []\n",
    "#     data = []\n",
    "#     for i in range(0,len(df[0])):\n",
    "#         try:\n",
    "#             review = df[0][i].split(' ')\n",
    "#             col = np.append(col,review)\n",
    "#             wc = np.full(len(review),i)\n",
    "#             row = np.append(row, wc)\n",
    "#             data = np.append(data,np.full(len(review),1/len(review)))\n",
    "#         except:\n",
    "#             continue\n",
    "    \n",
    "#     col = list(map(int, col))\n",
    "#     vMatrix = coo_matrix((data, (row, col)), shape=(len(df), 10000))\n",
    "#     return vMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "def bow(df):\n",
    "    vMatrix = np.zeros((len(df), 10000))\n",
    "    for i in range(0,len(df[0])):\n",
    "        try:\n",
    "            review = df.iloc[i,0].split(' ')\n",
    "            review = list(map(int, review))\n",
    "            for j in range(0, len(review)):\n",
    "                vMatrix[i][review[j]] = 1\n",
    "        except:\n",
    "            continue\n",
    "    return vMatrix\n",
    "\n",
    "# bag of frequency\n",
    "def bof(df):\n",
    "    vMatrix = np.zeros((len(df), 10000))\n",
    "    for i in range(0,len(df[0])):\n",
    "        try:\n",
    "            comment = df.iloc[i,0].split(' ')\n",
    "            comment = list(map(int, comment))\n",
    "            for j in range(0, len(comment)):\n",
    "                vMatrix[i][comment[j]] = vMatrix[i][comment[j]]+ 1/len(comment)\n",
    "        except:\n",
    "            continue\n",
    "    return vMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize yelp data\n",
    "yTw = bow(YT)\n",
    "yVw = bow(YV)\n",
    "yTew = bow(YTe)\n",
    "yTf = bof(YT)\n",
    "yVf = bof(YV)\n",
    "yTef = bof(YTe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize IMDB data\n",
    "iTw = bow(IT)\n",
    "iVw = bow(IV)\n",
    "iTew = bow(ITe)\n",
    "iTf = bof(IT)\n",
    "iVf = bof(IV)\n",
    "iTef = bof(ITe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random classifier\n",
    "def rclf(nClass,x):\n",
    "    result = np.zeros(x.shape[0])\n",
    "    for i in range(0,x.shape[0]):\n",
    "        result[i] = np.floor(np.random.random() / (1/nClass)) + 1\n",
    "    return result\n",
    "\n",
    "# majority classifier\n",
    "def mclf(x,y):\n",
    "    data = Counter(y)\n",
    "    return np.full(x.shape[0],data.most_common(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "def nbclf(x,y,a,test):\n",
    "    clf = BernoulliNB(alpha = a)\n",
    "    clf.fit(x,y)\n",
    "    return clf.predict(test)\n",
    "\n",
    "def ngclf(x,y, test):\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(x, y)\n",
    "    return clf.predict(test)\n",
    "\n",
    "# linear SVM\n",
    "def sclf(x,y,c,test):\n",
    "    clf = LinearSVC(C=c, random_state=0)\n",
    "    clf.fit(x, y)\n",
    "    return clf.predict(test)\n",
    "\n",
    "# decision tree \n",
    "def dclf(x,y,md,test):\n",
    "    clf = DecisionTreeClassifier(max_depth=md, random_state=0)\n",
    "    clf.fit(x, y)\n",
    "    return clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random classifier f1-measure: 0.23746167753661265\n",
      "Majortity classifier f1-measure: 0.18238490007401925\n",
      "Random classifier f1-measure: 0.2506872783776246\n",
      "Majortity classifier f1-measure: 0.33333333333333326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/regulusyl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/regulusyl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# test random and majority classifier\n",
    "ry = f1_score(list(YTe[1]), rclf(5,yTew), average='weighted')\n",
    "my = f1_score(list(YTe[1]), mclf(yTew,YT[1]), average='weighted')\n",
    "\n",
    "ri = f1_score(list(ITe[1]), rclf(2,iTew), average='weighted')\n",
    "mi = f1_score(list(ITe[1]), mclf(iTew,IT[1]), average='weighted')\n",
    "\n",
    "print(\"Random classifier f1-measure:\", ry)\n",
    "print(\"Majority classifier f1-measure:\", my)\n",
    "print(\"Random classifier f1-measure:\", ri)\n",
    "print(\"Majority classifier f1-measure:\", mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/regulusyl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "# each parameter is tested with 10 values\n",
    "numP = 10\n",
    "nbb = np.zeros(numP)\n",
    "nbg = np.zeros(numP)\n",
    "sw = np.zeros(numP)\n",
    "sf = np.zeros(numP)\n",
    "dw = np.zeros(numP)\n",
    "df = np.zeros(numP)\n",
    "\n",
    "param = 0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000\n",
    "\n",
    "for i in range(0,numP):\n",
    "    nbb[i] = f1_score(list(YV[1]), nbclf(yTw,list(YT[1]),param[i],yVw), average='weighted')\n",
    "#     nbg[i] = f1_score(list(YV[1]), ngclf(yTf,list(YT[1]),param[i],yVf), average='weighted')\n",
    "    sw[i] = f1_score(list(YV[1]), sclf(yTw,list(YT[1]),param[i],yVw), average='weighted')\n",
    "    sf[i] = f1_score(list(YV[1]), sclf(yTf,list(YT[1]),param[i],yVf), average='weighted')\n",
    "    dw[i] = f1_score(list(YV[1]), dclf(yTw,list(YT[1]),param[i],yVw), average='weighted')\n",
    "    df[i] = f1_score(list(YV[1]), dclf(yTf,list(YT[1]),param[i],yVf), average='weighted')\n",
    "    \n",
    "bnbb = np.amax(nbb)\n",
    "# bnbg = np.amax(nbg)\n",
    "bnbg = f1_score(list(YV[1]), ngclf(yTf,list(YT[1]),yVf), average='weighted')\n",
    "bsw = np.amax(sw)\n",
    "bsf = np.amax(sf)\n",
    "bdw = np.amax(dw)\n",
    "bdf = np.amax(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbb = list(nbb)\n",
    "sw = list(sw)\n",
    "sf = list(sf)\n",
    "dw = list(dw)\n",
    "df = list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f1-measure\n",
      "Best Bernoulli Naive Bayes validation f1-measure: 0.413109637653621  Obtained at alpha =  0.01\n",
      "Best Gaussian Naive Bayes validation f1-measure: 0.29237439463776815\n",
      "Best SVM Bag of words validation f1-measure: 0.48780633736872003  Obtained at C =  0.01\n",
      "Best SVM Bag of frequency validation f1-measure: 0.4882976993661879  Obtained at C =  100\n",
      "Best Decision Tree Bag of words validation f1-measure: 0.34872356922850845  Obtained at max depth of  10\n",
      "Best Decision Tree Bag of frequency validation f1-measure: 0.3907368652210626  Obtained at max depth of  10\n",
      "Training f1-measure\n",
      "Bernoulli Naive Bayes training f1-measure at best alpha: 0.7502393423348799\n",
      "Gaussian Naive Bayes training f1-measure: 0.8077916059661376\n",
      "SVM Bag of words training f1-measure at best C: 0.8408856489275267\n",
      "SVM Bag of frequency training f1-measure at best C: 0.8795874257831433\n",
      "Decision Tree Bag of words training f1-measure at best max depth: 0.4981599437858736\n",
      "Decision Tree Bag of frequency training f1-measure at best max depth: 0.523773079475586\n",
      "Test f1-measure\n",
      "Bernoulli Naive Bayes test f1-measure at best alpha: 0.4281582001950195\n",
      "Gaussian Naive Bayes test f1-measure: 0.31125108465640666\n",
      "SVM Bag of words test f1-measure at best C: 0.4943772600025794\n",
      "SVM Bag of frequency test f1-measure at best C: 0.4958803703811586\n",
      "Decision Tree Bag of words test f1-measure at best max depth: 0.35160149692692766\n",
      "Decision Tree Bag of frequency test f1-measure at best max depth: 0.36699717695337747\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation f1-measure\")\n",
    "print(\"Best Bernoulli Naive Bayes validation f1-measure:\", bnbb, \" Obtained at alpha = \", param[nbb.index(bnbb)])\n",
    "print(\"Best Gaussian Naive Bayes validation f1-measure:\", bnbg)\n",
    "print(\"Best SVM Bag of words validation f1-measure:\", bsw, \" Obtained at C = \", param[sw.index(bsw)])\n",
    "print(\"Best SVM Bag of frequency validation f1-measure:\", bsf, \" Obtained at C = \", param[sf.index(bsf)])\n",
    "print(\"Best Decision Tree Bag of words validation f1-measure:\", bdw, \" Obtained at max depth of \", param[dw.index(bdw)])\n",
    "print(\"Best Decision Tree Bag of frequency validation f1-measure:\", bdf, \" Obtained at max depth of \", param[df.index(bdf)])\n",
    "\n",
    "print(\"Training f1-measure\")\n",
    "print(\"Bernoulli Naive Bayes training f1-measure at best alpha:\",f1_score(list(YT[1]), nbclf(yTw,list(YT[1]),param[nbb.index(bnbb)],yTw), average='weighted') )\n",
    "print(\"Gaussian Naive Bayes training f1-measure:\",f1_score(list(YT[1]), ngclf(yTf,list(YT[1]),yTf), average='weighted'))\n",
    "print(\"SVM Bag of words training f1-measure at best C:\",f1_score(list(YT[1]), sclf(yTw,list(YT[1]),param[sw.index(bsw)],yTw), average='weighted'))\n",
    "print(\"SVM Bag of frequency training f1-measure at best C:\",f1_score(list(YT[1]), sclf(yTf,list(YT[1]),param[sf.index(bsf)],yTf), average='weighted'))\n",
    "print(\"Decision Tree Bag of words training f1-measure at best max depth:\",f1_score(list(YT[1]), dclf(yTw,list(YT[1]),param[dw.index(bdw)],yTw), average='weighted'))\n",
    "print(\"Decision Tree Bag of frequency training f1-measure at best max depth:\", f1_score(list(YT[1]), dclf(yTf,list(YT[1]),param[df.index(bdf)],yTf), average='weighted'))\n",
    "\n",
    "print(\"Test f1-measure\")\n",
    "print(\"Bernoulli Naive Bayes test f1-measure at best alpha:\",f1_score(list(YTe[1]), nbclf(yTw,list(YT[1]),param[nbb.index(bnbb)],yTew), average='weighted') )\n",
    "print(\"Gaussian Naive Bayes test f1-measure:\",f1_score(list(YTe[1]), ngclf(yTf,list(YT[1]),yTef), average='weighted'))\n",
    "print(\"SVM Bag of words test f1-measure at best C:\",f1_score(list(YTe[1]), sclf(yTw,list(YT[1]),param[sw.index(bsw)],yTew), average='weighted'))\n",
    "print(\"SVM Bag of frequency test f1-measure at best C:\",f1_score(list(YTe[1]), sclf(yTf,list(YT[1]),param[sf.index(bsf)],yTef), average='weighted'))\n",
    "print(\"Decision Tree Bag of words test f1-measure at best max depth:\",f1_score(list(YTe[1]), dclf(yTw,list(YT[1]),param[dw.index(bdw)],yTew), average='weighted'))\n",
    "print(\"Decision Tree Bag of frequency test f1-measure at best max depth:\", f1_score(list(YTe[1]), dclf(yTf,list(YT[1]),param[df.index(bdf)],yTef), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/regulusyl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "# each parameter is tested with 100000 values\n",
    "numP = 10\n",
    "nbb = np.zeros(numP)\n",
    "nbg = np.zeros(numP)\n",
    "sw = np.zeros(numP)\n",
    "sf = np.zeros(numP)\n",
    "dw = np.zeros(numP)\n",
    "df = np.zeros(numP)\n",
    "\n",
    "param = 0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000\n",
    "\n",
    "# naive bayes\n",
    "for i in range(0,numP):\n",
    "    nbb[i] = f1_score(list(IV[1]), nbclf(iTw,list(IT[1]),param[i],iVw), average='weighted')\n",
    "#     nbg[i] = f1_score(list(IV[1]), ngclf(iTf,list(IT[1]),param[i],iVf), average='weighted')\n",
    "    sw[i] = f1_score(list(IV[1]), sclf(iTw,list(IT[1]),param[i],iVw), average='weighted')\n",
    "    sf[i] = f1_score(list(IV[1]), sclf(iTf,list(IT[1]),param[i],iVf), average='weighted')\n",
    "    dw[i] = f1_score(list(IV[1]), dclf(iTw,list(IT[1]),param[i],iVw), average='weighted')\n",
    "    df[i] = f1_score(list(IV[1]), dclf(iTf,list(IT[1]),param[i],iVf), average='weighted')\n",
    "    \n",
    "bnbb = np.amax(nbb)\n",
    "# bnbg = np.amax(nbg)\n",
    "bnbg = f1_score(list(IV[1]), ngclf(iTf,list(IT[1]),iVf), average='weighted')\n",
    "bsw = np.amax(sw)\n",
    "bsf = np.amax(sf)\n",
    "bdw = np.amax(dw)\n",
    "bdf = np.amax(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbb = list(nbb)\n",
    "sw = list(sw)\n",
    "sf = list(sf)\n",
    "dw = list(dw)\n",
    "df = list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation f-measure\n",
      "Best Bernoulli Naive Bayes validation f-measure: 0.8426650700722068  Obtained at alpha =  0.1\n",
      "Best Gaussian Naive Bayes validation f-measure: 0.7513922734965905\n",
      "Best SVM Bag of words validation f-measure: 0.8747957881303126  Obtained at C =  0.01\n",
      "Best SVM Bag of frequency validation f-measure: 0.8785929875309597  Obtained at C =  100\n",
      "Best Decision Tree Bag of words validation f-measure: 0.7105956664444624  Obtained at max depth of  10\n",
      "Best Decision Tree Bag of frequency validation f-measure: 0.701489066096874  Obtained at max depth of  10\n",
      "Training f-measure\n",
      "Bernoulli Naive Bayes training f-measure at best alpha: 0.870294063564018\n",
      "Gaussian Naive Bayes training f-measure: 0.8621070566038361\n",
      "SVM Bag of words training f-measure at best C: 0.9632663360636912\n",
      "SVM Bag of frequency training f-measure at best C: 0.9460661371559866\n",
      "Decision Tree Bag of words training f-measure at best max depth: 0.7594146771119565\n",
      "Decision Tree Bag of frequency training f-measure at best max depth: 0.7662780306255659\n",
      "Test f-measure\n",
      "Bernoulli Naive Bayes test f-measure at best alpha: 0.5283347652692814\n",
      "Gaussian Naive Bayes test f-measure: 0.6862657359207126\n",
      "SVM Bag of words test f-measure at best C: 0.8692383427790609\n",
      "SVM Bag of frequency test f-measure at best C: 0.8741199981873279\n",
      "Decision Tree Bag of words test f-measure at best max depth: 0.7124566274296956\n",
      "Decision Tree Bag of frequency test f-measure at best max depth: 0.7083284152893591\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation f-measure\")\n",
    "print(\"Best Bernoulli Naive Bayes validation f-measure:\", bnbb, \" Obtained at alpha = \", param[nbb.index(bnbb)])\n",
    "print(\"Best Gaussian Naive Bayes validation f-measure:\", bnbg)\n",
    "print(\"Best SVM Bag of words validation f-measure:\", bsw, \" Obtained at C = \", param[sw.index(bsw)])\n",
    "print(\"Best SVM Bag of frequency validation f-measure:\", bsf, \" Obtained at C = \", param[sf.index(bsf)])\n",
    "print(\"Best Decision Tree Bag of words validation f-measure:\", bdw, \" Obtained at max depth of \", param[dw.index(bdw)])\n",
    "print(\"Best Decision Tree Bag of frequency validation f-measure:\", bdf, \" Obtained at max depth of \", param[df.index(bdf)])\n",
    "\n",
    "print(\"Training f-measure\")\n",
    "print(\"Bernoulli Naive Bayes training f-measure at best alpha:\",f1_score(list(IT[1]), nbclf(iTw,list(IT[1]),param[nbb.index(bnbb)],iTw), average='weighted') )\n",
    "print(\"Gaussian Naive Bayes training f-measure:\",f1_score(list(IT[1]), ngclf(iTf,list(IT[1]),iTf), average='weighted'))\n",
    "print(\"SVM Bag of words training f-measure at best C:\",f1_score(list(IT[1]), sclf(iTw,list(IT[1]),param[sw.index(bsw)],iTw), average='weighted'))\n",
    "print(\"SVM Bag of frequency training f-measure at best C:\",f1_score(list(IT[1]), sclf(iTf,list(IT[1]),param[sf.index(bsf)],iTf), average='weighted'))\n",
    "print(\"Decision Tree Bag of words training f-measure at best max depth:\",f1_score(list(IT[1]), dclf(iTw,list(IT[1]),param[dw.index(bdw)],iTw), average='weighted'))\n",
    "print(\"Decision Tree Bag of frequency training f-measure at best max depth:\", f1_score(list(IT[1]), dclf(iTf,list(IT[1]),param[df.index(bdf)],iTf), average='weighted'))\n",
    "\n",
    "print(\"Test f-measure\")\n",
    "print(\"Bernoulli Naive Bayes test f-measure at best alpha:\",f1_score(list(ITe[1]), nbclf(iTw,list(IT[1]),param[i],iTew), average='weighted') )\n",
    "print(\"Gaussian Naive Bayes test f-measure:\",f1_score(list(ITe[1]), ngclf(iTf,list(IT[1]),iTef), average='weighted'))\n",
    "print(\"SVM Bag of words test f-measure at best C:\",f1_score(list(ITe[1]), sclf(iTw,list(IT[1]),param[sw.index(bsw)],iTew), average='weighted'))\n",
    "print(\"SVM Bag of frequency test f-measure at best C:\",f1_score(list(ITe[1]), sclf(iTf,list(IT[1]),param[sf.index(bsf)],iTef), average='weighted'))\n",
    "print(\"Decision Tree Bag of words test f-measure at best max depth:\",f1_score(list(ITe[1]), dclf(iTw,list(IT[1]),param[dw.index(bdw)],iTew), average='weighted'))\n",
    "print(\"Decision Tree Bag of frequency test f-measure at best max depth:\", f1_score(list(ITe[1]), dclf(iTf,list(IT[1]),param[df.index(bdf)],iTef), average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
